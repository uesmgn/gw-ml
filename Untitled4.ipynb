{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from attrdict import AttrDict as attrdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import datasets.utils as du\n",
    "import net.models as models\n",
    "from utils.clustering import decomposition, metrics, functional\n",
    "from utils.plotlib import plot\n",
    "\n",
    "ROOT = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "def downsample(in_planes, out_planes, stride=2):\n",
    "    return nn.Sequential(\n",
    "                conv1x1(in_planes, out_planes, stride=2),\n",
    "                nn.BatchNorm2d(out_planes)\n",
    "           )\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv3x3(in_planes, out_planes, stride),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(out_planes, out_planes),\n",
    "            nn.BatchNorm2d(out_planes)\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        features = self.features(x)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        features += identity\n",
    "        return F.relu(features, inplace=True)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, num_classes, block, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(num_blocks) == 4\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2_x = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            self._make_layer(64, 64, block, num_blocks[0])\n",
    "        )\n",
    "\n",
    "        self.conv3_x = nn.Sequential(\n",
    "            self._make_layer(64, 128, block, num_blocks[1], stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv4_x = nn.Sequential(\n",
    "            self._make_layer(128, 256, block, num_blocks[2], stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv5_x = nn.Sequential(\n",
    "            self._make_layer(256, 512, block, num_blocks[3], stride=2)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _make_layer(self, in_planes, out_planes, block, num_block, stride=1):\n",
    "        assert num_block > 0\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        if stride > 1:\n",
    "            downsample = nn.Sequential(\n",
    "                            conv1x1(in_planes, out_planes, stride=stride),\n",
    "                            nn.BatchNorm2d(out_planes)\n",
    "                        )\n",
    "        layers.append(block(in_planes, out_planes, stride=stride, downsample=downsample))\n",
    "\n",
    "        for _ in range(1, num_block):\n",
    "            layers.append(block(out_planes, out_planes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Gaussian(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.features = nn.Linear(in_dim, out_dim * 2)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, reparameterize=True):\n",
    "        x = self.features(x)\n",
    "        mean, logit = torch.split(x, x.shape[1] // 2, -1)\n",
    "        var = F.softplus(logit) + self.eps\n",
    "        if reparameterize:\n",
    "            x = self._reparameterize(mean, var)\n",
    "        else:\n",
    "            x = mean\n",
    "        return x, mean, var\n",
    "    \n",
    "    def _reparameterize(self, mean, var):\n",
    "        if torch.is_tensor(var):\n",
    "            std = torch.pow(var, 0.5)\n",
    "        else:\n",
    "            std = np.sqrt(var)\n",
    "        eps = torch.randn_like(mean)\n",
    "        x = mean + eps * std\n",
    "        return x\n",
    "\n",
    "class GumbelSoftmax(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.features = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, tau=1., dim=-1, hard=False):\n",
    "        logits = self.features(x)\n",
    "        pi = logits.softmax(dim)\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "        y = gumbels.softmax(dim)\n",
    "\n",
    "        if hard:\n",
    "            index = y.max(dim, keepdim=True)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(dim, index, 1.0)\n",
    "            y = y_hard - y.detach() + y\n",
    "\n",
    "        return logits, y\n",
    "    \n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, outer_shape):\n",
    "        super().__init__()\n",
    "        self.outer_shape = outer_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), *self.outer_shape)\n",
    "\n",
    "def bce_loss(inputs, targets):\n",
    "    loss = F.binary_cross_entropy(inputs, targets, reduction='none').view(inputs.shape[0], -1).sum(-1)\n",
    "    return loss\n",
    "\n",
    "def _log_norm(x, mean=None, var=None):\n",
    "    if mean is None:\n",
    "        mean = torch.zeros_like(x)\n",
    "    if var is None:\n",
    "        var = torch.ones_like(x)\n",
    "    return -0.5 * (torch.log(2.0 * np.pi * var) + torch.pow(x - mean, 2) / var )\n",
    "\n",
    "def log_norm_kl(x, mean, var, mean_=None, var_=None):\n",
    "    log_p = _log_norm(x, mean, var).sum(-1)\n",
    "    log_q = _log_norm(x, mean_, var_).sum(-1)\n",
    "    loss = log_p - log_q\n",
    "    return loss\n",
    "\n",
    "def entropy(logits):\n",
    "    p = logits.softmax(-1)\n",
    "    log_p = logits.log_softmax(-1)\n",
    "    entropy = -(p * log_p).sum(-1)\n",
    "    return entropy\n",
    "\n",
    "class ResNet_VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet = ResNet(1, 1000, BasicBlock, [3, 3, 3, 3])\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.gaussian = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256, momentum=0.01),\n",
    "            Gaussian(256, 64)\n",
    "        )\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Linear(64, 512),\n",
    "            nn.BatchNorm1d(512, momentum=0.01),\n",
    "            nn.Linear(512, 512 * 64),\n",
    "            nn.BatchNorm1d(512 * 64, momentum=0.01),\n",
    "            Reshape((512, 8, 8))\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(256, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(128, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(64, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=7, stride=3, padding=2),\n",
    "            nn.BatchNorm2d(1, momentum=0.01),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x, return_params=False):\n",
    "        z, z_mean, z_var = self.gaussian(self.encoder(x))\n",
    "        x_reconst = self.decoder(self.upsample(z))\n",
    "        if return_params:\n",
    "            return {'x': x,'x_reconst': x_reconst, 'z': z, 'z_mean': z_mean, 'z_var': z_var}\n",
    "        return x_reconst\n",
    "    \n",
    "    def criterion(self, x, params):\n",
    "        loss = bce_loss(params['x_reconst'], x)\n",
    "        kl = log_norm_kl(params['z'], params['z_mean'], params['z_var'])\n",
    "        return (loss + kl).mean()\n",
    "\n",
    "class ResNet_CVAE(nn.Module):\n",
    "    def __init__(self, z_dim=64, y_dim=10):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet = ResNet(1, 1000, BasicBlock, [3, 3, 3, 3])\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.inference_y = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256, momentum=0.01),\n",
    "            GumbelSoftmax(256, y_dim)\n",
    "        )\n",
    "        self.inference_z_prior = nn.Sequential(\n",
    "            nn.Linear(y_dim, 256),\n",
    "            nn.BatchNorm1d(256, momentum=0.01),\n",
    "            Gaussian(256, z_dim)\n",
    "        )\n",
    "        self.inference_z = nn.Sequential(\n",
    "            nn.Linear(512 + y_dim, 256),\n",
    "            nn.BatchNorm1d(256, momentum=0.01),\n",
    "            Gaussian(256, z_dim)\n",
    "        )\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Linear(64, 512),\n",
    "            nn.BatchNorm1d(512, momentum=0.01),\n",
    "            nn.Linear(512, 512 * 64),\n",
    "            nn.BatchNorm1d(512 * 64, momentum=0.01),\n",
    "            Reshape((512, 8, 8))\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(256, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(128, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(64, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=7, stride=3, padding=2),\n",
    "            nn.BatchNorm2d(1, momentum=0.01),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x, return_params=False):\n",
    "        h = self.encoder(x)\n",
    "        y_logits, y = self.inference_y(h)\n",
    "        _, y_pred = torch.max(y_logits, -1)\n",
    "        _, z_prior_mean, z_prior_var = self.inference_z_prior(y)\n",
    "        xy = torch.cat((h, y),-1)\n",
    "        z, z_mean, z_var = self.inference_z(xy)\n",
    "        x_reconst = self.decoder(self.upsample(z))\n",
    "        if return_params:\n",
    "            return dict(\n",
    "                x=x,\n",
    "                x_reconst=x_reconst,\n",
    "                y_logits=y_logits, \n",
    "                y=y,\n",
    "                y_pred=y_pred,\n",
    "                z=z, \n",
    "                z_mean=z_mean, \n",
    "                z_var=z_var,\n",
    "                z_prior_mean=z_prior_mean, \n",
    "                z_prior_var=z_prior_var\n",
    "            )\n",
    "        return x_reconst\n",
    "    \n",
    "    def criterion(self, x, params):\n",
    "        loss = bce_loss(params['x_reconst'], x)\n",
    "        z_kl = log_norm_kl(params['z'], params['z_mean'], params['z_var'], params['z_prior_mean'], params['z_prior_var'])\n",
    "        y_en = entropy(params['y_logits'])\n",
    "        return (loss + z_kl + y_en).mean()\n",
    "\n",
    "FLAGS = attrdict(\n",
    "    x_dim=486,\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    num_workers=4,\n",
    "    log_steps=1,\n",
    "    lr=1e-2,\n",
    "    dataset='gravityspy'\n",
    ")\n",
    "\n",
    "\n",
    "setup_transform = transforms.Compose([\n",
    "                transforms.CenterCrop(FLAGS.x_dim),\n",
    "                transforms.Grayscale(),\n",
    "                transforms.Resize(213),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "                transforms.Grayscale(),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "dataset = getattr(datasets, FLAGS.dataset)(root=ROOT, transform=data_transform, \n",
    "                                                                              setup_transform=setup_transform, \n",
    "                                                                              download=True)\n",
    "dataset.get_by_keys([3, 9, 14])\n",
    "print(f'dataset length: {len(dataset)}')\n",
    "loader = DataLoader(dataset,\n",
    "                    batch_size=FLAGS.batch_size,\n",
    "                    num_workers=FLAGS.num_workers,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True)\n",
    "    \n",
    "\n",
    "y_dim = 3\n",
    "\n",
    "device_ids = range(torch.cuda.device_count())\n",
    "device = f'cuda:{device_ids[0]}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = ResNet_CVAE(y_dim=y_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=FLAGS.lr)\n",
    "\n",
    "for epoch in range(1, FLAGS.num_epochs):\n",
    "    loss = 0\n",
    "    features = torch.Tensor().to(device)\n",
    "    target_stack = np.array([]).astype(np.int)\n",
    "    pred_stack = np.array([]).astype(np.int)\n",
    "    for step, (x, target) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        out_params = model(x, return_params=True)\n",
    "        step_loss = model.criterion(x, out_params)\n",
    "        optimizer.zero_grad()\n",
    "        step_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += step_loss.item()\n",
    "\n",
    "        features = torch.cat((features, out_params['z']), 0)\n",
    "        target_stack = np.append(target_stack, list(target.numpy()))\n",
    "        pred_stack = np.append(pred_stack, list(out_params['y_pred'].detach().cpu().numpy()))\n",
    "    \n",
    "    features = features.detach().cpu().numpy()\n",
    "    km_stack = functional.run_kmeans(features, y_dim + 3)\n",
    "\n",
    "    print(f'loss: {loss:.3f} at epoch {epoch}')\n",
    "\n",
    "    if epoch % FLAGS.log_steps == 0:\n",
    "\n",
    "        grid_img = make_grid(x.cpu()[:16], nrow=8)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(grid_img.permute(1, 2, 0))\n",
    "        plt.show()\n",
    "\n",
    "        grid_img = make_grid(out_params['x_reconst'].detach().cpu()[:16], nrow=8)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(grid_img.permute(1, 2, 0))\n",
    "        plt.show()\n",
    "\n",
    "        tsne = decomposition.TSNE()\n",
    "        features = tsne.fit_transform(features)\n",
    "        plot.scatter(features[:,0], features[:,1], target_stack)\n",
    "        plot.scatter(features[:,0], features[:,1], pred_stack)\n",
    "        plot.scatter(features[:,0], features[:,1], km_stack)\n",
    "        \n",
    "        pca = decomposition.PCA()\n",
    "        features = pca.fit_transform(features)\n",
    "        plot.scatter(features[:,0], features[:,1], target_stack)\n",
    "        plot.scatter(features[:,0], features[:,1], pred_stack)\n",
    "        plot.scatter(features[:,0], features[:,1], km_stack)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 40)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "mt = np.random.rand(1000, 40).astype('float32')\n",
    "print(mt.shape)\n",
    "mat = faiss.PCAMatrix (40, 2)\n",
    "mat.train(mt)\n",
    "tr = mat.apply_py(mt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
