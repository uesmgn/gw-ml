{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_batch = 32\n",
    "w_dim = 20\n",
    "K = 16\n",
    "eps = 1e-10\n",
    "mean1 = t.zeros(n_batch, w_dim)\n",
    "mean2 = t.zeros(n_batch, w_dim)\n",
    "var1 = t.ones(n_batch, w_dim) + eps\n",
    "var2 =  t.ones(n_batch, w_dim) + eps\n",
    "\n",
    "var1.shape\n",
    "# var1.unsqueeze(-1).repeat(1,2).shape\n",
    "# var3 = var1.repeat(2, K)\n",
    "# var3.shape\n",
    "\n",
    "mean_copies = mean1.unsqueeze(-1).repeat(1, 1, K)\n",
    "var_copies = var1.unsqueeze(-1).repeat(1, 1, K)\n",
    "means = t.zeros(n_batch, w_dim, K)\n",
    "varss = t.ones(n_batch, w_dim, K)\n",
    "(t.log(varss/var_copies) + var_copies/varss + t.pow(mean_copies - means, 2) / varss - 1).sum(1).shape\n",
    "\n",
    "# b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.sigmoid(t.randn(16, 486, 486))\n",
    "x_ = t.sigmoid(t.randn(12, 486, 486))\n",
    "x.shape == x_.shape\n",
    "# F.binary_cross_entropy(x_, x, reduction='none').sum(1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2893), tensor(0.2893))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(16)\n",
    "b = t.randn(16)\n",
    "c = a.mean() + b.mean()\n",
    "d = (a + b).mean()\n",
    "c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0972e+03, 1.4077e+02, 1.3074e+01, 4.2247e-01])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def binary_cross_entropy(x, x_):\n",
    "    # x: (batch_size, x_size, x_size)\n",
    "    # x_: (batch_size, x_size, x_size)\n",
    "    # loss: (batch_size, )\n",
    "    assert x.shape == x_.shape\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    x_ = x_.view(x_.shape[0], -1)\n",
    "    loss = F.binary_cross_entropy(x_, x, reduction='none').sum(1)\n",
    "    return loss\n",
    "\n",
    "def gaussian_gmm_kl(mean, var, means, variances, pi):\n",
    "    # mean: (batch_size, dim)\n",
    "    # var: (batch_size, dim) > 0\n",
    "    # means: (batch_size, dim, K)\n",
    "    # vars: (batch_size, dim, K) > 0\n",
    "    # pi: (batch_size, K)\n",
    "    # kl: (batch_size, )\n",
    "    K = pi.shape[-1]\n",
    "    mean_repeat = mean.unsqueeze(-1).repeat(1, 1, K)\n",
    "    var_repeat = var.unsqueeze(-1).repeat(1, 1, K)\n",
    "    kl = (pi * gaussian_kl(mean_repeat, var_repeat, means, variances)).sum(1)\n",
    "    return kl\n",
    "\n",
    "def gaussian_kl(mean1, var1, mean2, var2):\n",
    "    # mean1: (batch_size, dim, .. )\n",
    "    # mean2: (batch_size, dim, .. )\n",
    "    # var1: (batch_size, dim, .. ) > 0\n",
    "    # var2: (batch_size, dim, .. ) > 0\n",
    "    # kl: (batch_size, .. )\n",
    "    assert (torch.cat([var1, var2]) > 0).all()\n",
    "    kl = (torch.log(var2 / var1) + var1 / var2 + torch.pow(mean1 - mean2, 2) / var2 - 1).sum(1)\n",
    "    return kl\n",
    "\n",
    "def standard_gaussian_kl(mean, var):\n",
    "    # mean: (batch_size, dim, ..)\n",
    "    # var: (batch_size, dim, ..)\n",
    "    # kl: (batch_size, ..)\n",
    "    kl = 0.5 * (var - 1 - torch.log(var) + torch.pow(mean, 2)).sum(1)\n",
    "    return kl\n",
    "\n",
    "def uniform_categorical_kl(y):\n",
    "    # y: (batch_size, K)\n",
    "    # kl: (batch_size, )\n",
    "    k = y.shape[-1]\n",
    "    u = torch.ones_like(y) / k\n",
    "    kl = F.kl_div(torch.log(u), y,reduction='none').sum(1)\n",
    "    return kl\n",
    "\n",
    "b = 16\n",
    "x_dim = 486\n",
    "w_dim = 20\n",
    "y_dim = 16\n",
    "z_dim = 32\n",
    "x = t.sigmoid(100*t.randn(b, x_dim, x_dim))\n",
    "x_z = x\n",
    "w_x_mean = t.randn(b, w_dim)\n",
    "w_x_var = F.softplus(t.randn(b, w_dim))\n",
    "y_wz = F.softmax(t.randn(b, y_dim), dim=1)\n",
    "z_x_mean = t.randn(b, z_dim)\n",
    "z_x_var = F.softplus(t.randn(b, z_dim))\n",
    "z_wy_means = t.randn(b, z_dim, y_dim)\n",
    "z_wy_vars = F.softplus(t.randn(b, z_dim, y_dim))\n",
    "\n",
    "\n",
    "rec_loss = binary_cross_entropy(x, x_z).mean()\n",
    "cond_kl = gaussian_gmm_kl(z_x_mean, z_x_var, z_wy_means, z_wy_vars, y_wz).mean()\n",
    "w_kl = standard_gaussian_kl(w_x_mean, w_x_var).mean()\n",
    "y_kl = uniform_categorical_kl(y_wz).mean()\n",
    "\n",
    "total = t.cat([rec_loss.view(-1),\n",
    "                        cond_kl.view(-1),\n",
    "                        w_kl.view(-1),\n",
    "                        y_kl.view(-1)])\n",
    "\n",
    "total\n",
    "# t.sigmoid(t.Tensor([1,4,3,2,5,2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "a.append(None)\n",
    "a.append(None)\n",
    "[i for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9274)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.empty(1000).exponential_()\n",
    "a.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3026)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "eps = 1e-10\n",
    "inputs = F.softmax(torch.Tensor([1,1,1,1,1,1,1,1,1,1]), -1)\n",
    "targets = F.gumbel_softmax(torch.Tensor([0,0,0,0,0,0,0,0,0,1]), dim=-1, tau=.1)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "(inputs * torch.log(inputs/targets + eps)).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
