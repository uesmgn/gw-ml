{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "def make_spiral_moons(n_samples=10000, k=4, noise=0.1, imbalance=True, padding=0.3, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    x_stack = np.array([])\n",
    "    y_stack = np.array([])\n",
    "    labels = np.array([])    \n",
    "    \n",
    "    if imbalance:\n",
    "        sizes = _random_split(n_samples, k, random_state=random_state)\n",
    "    else:\n",
    "        eps = np.append(np.ones(n_samples % k), np.zeros(k - n_samples % k))\n",
    "        eps = np.random.permutation(eps).astype(np.int)\n",
    "        sizes = [n_samples // k + eps[i] for i in range(k-1)]\n",
    "        sizes.append(n_samples - np.sum(sizes))\n",
    "    for i in range(k):\n",
    "        size = sizes[i]\n",
    "        x = np.random.normal(loc=np.pi/2+padding, scale=0.5, size=size)\n",
    "        sin_gauss = np.sin(np.linspace(0, np.pi, size)) * (np.random.normal(loc=0, scale=noise, size=size))\n",
    "        y = np.sin(x - padding) - .2 + sin_gauss\n",
    "        theta = 2*np.pi * i / k \n",
    "        x_ = np.cos(theta)*x - np.sin(theta)*y\n",
    "        y_ = np.sin(theta)*x + np.cos(theta)*y\n",
    "        label = (np.ones(len(x_)) * i).astype(np.int)\n",
    "        x_stack = np.append(x_stack, x_)\n",
    "        y_stack = np.append(y_stack, y_)\n",
    "        labels = np.append(labels, label)\n",
    "    x_stack = np.ravel(x_stack)\n",
    "    y_stack = np.ravel(y_stack)\n",
    "    labels = np.ravel(labels)\n",
    "    return x_stack, y_stack, labels\n",
    "  \n",
    "def _random_split(n_samples, k, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    div = np.random.choice(list(range(1, n_samples-1)), k-1, replace=False).astype(np.int)\n",
    "    div = np.sort(div)\n",
    "    div = np.append(div, n_samples)\n",
    "    ret = []\n",
    "    x = 0\n",
    "    for i in range(k):\n",
    "        x_ = div[i] - x\n",
    "        ret.append(x_)\n",
    "        x = div[i]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y, labels = make_spiral_moons(k=8, noise=0.2, imbalance=False)\n",
    "data = np.stack((x, y), -1).astype(np.float32)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40184309482574465\n",
      "159.65625\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import time\n",
    "\n",
    "dim=2\n",
    "# make index using brute-force L2 distance searching algorithm\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(data)    \n",
    "\n",
    "\n",
    "\n",
    "def trial():\n",
    "    k = 100    # we want to see 4 nearest neighbors\n",
    "    n_epoch = 10\n",
    "    total = 0\n",
    "    for i in range(n_epoch):\n",
    "        start = time.time()\n",
    "        D, I = index.search(data, k) # D: distance between each data points and top k neibors, I: index of top k neibors including original data\n",
    "        ellapse = time.time() - start\n",
    "        total += ellapse\n",
    "    print(total / n_epoch)\n",
    "\n",
    "memory_out = memory_usage(trial)\n",
    "print(np.mean(memory_out))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04367564201354981\n",
      "146.9605129076087\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nlist = 10\n",
    "k = 10\n",
    "quantizer = faiss.IndexFlatL2(dim)  # \n",
    "index = faiss.IndexIVFFlat(quantizer, dim, nlist) # Voronoi\n",
    "assert not index.is_trained\n",
    "index.train(data)\n",
    "assert index.is_trained\n",
    "index.add(data)\n",
    "\n",
    "\n",
    "def trial():\n",
    "    n_epoch = 100\n",
    "    total = 0\n",
    "    for i in range(n_epoch):\n",
    "        start = time.time()\n",
    "        D, I = index.search(data, k) # D: distance between each data points and top k neibors, I: index of top k neibors including original data\n",
    "        ellapse = time.time() - start\n",
    "        total += ellapse\n",
    "    print(total / n_epoch)\n",
    "\n",
    "memory_out = memory_usage(trial)\n",
    "print(np.mean(memory_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03906921863555908\n",
      "[138.81640625, 138.81640625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625, 134.97265625]\n"
     ]
    }
   ],
   "source": [
    "m = 2\n",
    "index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, 8)\n",
    "assert not index.is_trained\n",
    "index.train(data)\n",
    "assert index.is_trained\n",
    "index.add(data)\n",
    "\n",
    "def trial():\n",
    "    n_epoch = 100\n",
    "    total = 0\n",
    "    for i in range(n_epoch):\n",
    "        start = time.time()\n",
    "        D, I = index.search(data, k) # D: distance between each data points and top k neibors, I: index of top k neibors including original data\n",
    "        ellapse = time.time() - start\n",
    "        total += ellapse\n",
    "    print(total / n_epoch)\n",
    "    \n",
    "memory_out = memory_usage(trial)\n",
    "print(memory_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FaissKNNClassifier:\n",
    "    \"\"\" Scikit-learn wrapper interface for Faiss KNN.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_neighbors : int (Default = 5)\n",
    "                Number of neighbors used in the nearest neighbor search.\n",
    "    n_jobs : int (Default = None)\n",
    "             The number of jobs to run in parallel for both fit and predict.\n",
    "              If -1, then the number of jobs is set to the number of cores.\n",
    "    algorithm : {'brute', 'voronoi'} (Default = 'brute')\n",
    "        Algorithm used to compute the nearest neighbors:\n",
    "            - 'brute' will use the :class: `IndexFlatL2` class from faiss.\n",
    "            - 'voronoi' will use :class:`IndexIVFFlat` class from faiss.\n",
    "            - 'hierarchical' will use :class:`IndexHNSWFlat` class from faiss.\n",
    "        Note that selecting 'voronoi' the system takes more time during\n",
    "        training, however it can significantly improve the search time\n",
    "        on inference. 'hierarchical' produce very fast and accurate indexes,\n",
    "        however it has a higher memory requirement. It's recommended when\n",
    "        you have a lots of RAM or the dataset is small.\n",
    "        For more information see: https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n",
    "    n_cells : int (Default = 100)\n",
    "        Number of voronoi cells. Only used when algorithm=='voronoi'.\n",
    "    n_probes : int (Default = 1)\n",
    "        Number of cells that are visited to perform the search. Note that the\n",
    "        search time roughly increases linearly with the number of probes.\n",
    "        Only used when algorithm=='voronoi'.\n",
    "    References\n",
    "    ----------\n",
    "    Johnson Jeff, Matthijs Douze, and Hervé Jégou. \"Billion-scale similarity\n",
    "    search with gpus.\" arXiv preprint arXiv:1702.08734 (2017).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_neighbors=5,\n",
    "                 n_jobs=None,\n",
    "                 algorithm='brute',\n",
    "                 n_cells=100,\n",
    "                 n_probes=1):\n",
    "\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.n_jobs = n_jobs\n",
    "        self.algorithm = algorithm\n",
    "        self.n_cells = n_cells\n",
    "        self.n_probes = n_probes\n",
    "\n",
    "        import faiss\n",
    "        self.faiss = faiss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class label for each sample in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array, shape (n_samples,)\n",
    "                Class labels for samples in X.\n",
    "        \"\"\"\n",
    "        idx = self.kneighbors(X, self.n_neighbors, return_distance=False)\n",
    "        class_idx = self.y_[idx]\n",
    "        counts = np.apply_along_axis(\n",
    "            lambda x: np.bincount(x, minlength=self.n_classes_), axis=1,\n",
    "            arr=class_idx.astype(np.int16))\n",
    "        preds = np.argmax(counts, axis=1)\n",
    "        return preds\n",
    "\n",
    "    def kneighbors(self, X, n_components=None, return_distance=True):\n",
    "        n_components = n_components or self.n_components\n",
    "\n",
    "        elif n_components <= 0:\n",
    "            raise ValueError(\"Expected n_components > 0. Got {}\".format(n_components))\n",
    "        else:\n",
    "            if not np.issubdtype(type(n_components), np.integer):\n",
    "                raise TypeError(\"n_components does not take {} value, enter integer value\".format(type(n_components)))\n",
    "\n",
    "        check_is_fitted(self, 'index_')\n",
    "\n",
    "        X = np.atleast_2d(X).astype(np.float32)\n",
    "        dist, idx = self.index_.search(X, n_neighbors)\n",
    "        if return_distance:\n",
    "            return dist, idx\n",
    "        else:\n",
    "            return idx\n",
    "\n",
    "    def predict(self, X):\n",
    "        idx = self.kneighbors(X, self.n_neighbors, return_distance=False)\n",
    "        class_idx = self.y_[idx]\n",
    "        counts = np.apply_along_axis(\n",
    "            lambda x: np.bincount(x, minlength=self.n_classes_), axis=1,\n",
    "            arr=class_idx.astype(np.int16))\n",
    "\n",
    "        preds = counts / self.n_neighbors\n",
    "\n",
    "        return preds_proba\n",
    "\n",
    "    def fit(self, X, labels):\n",
    "        X = np.atleast_2d(X).astype(np.float32)\n",
    "        X = np.ascontiguousarray(X)\n",
    "        self.index_ = self._get_index(X)\n",
    "        self.index_.add(X)\n",
    "        self.labels_ = labels\n",
    "        self.n_classes_ = len(np.unique(labels))\n",
    "        return self\n",
    "\n",
    "    def _get_index(self, X):\n",
    "        dim = X.shape[1]\n",
    "        if self.algorithm == 'brute':\n",
    "            index = self.faiss.IndexFlatL2(d)\n",
    "        elif self.algorithm == 'voronoi':\n",
    "            quantizer = self.faiss.IndexFlatL2(dim)\n",
    "            index = self.faiss.IndexIVFFlat(quantizer, dim, self.ncells)\n",
    "            index.train(X)\n",
    "            index.nprobe = self.nprobe\n",
    "        elif self.algorithm == 'hierarchical':\n",
    "            index = self.faiss.IndexHNSWFlat(dim, 32)\n",
    "            index.hnsw.efConstruction = 40\n",
    "        else:\n",
    "            raise ValueError(\"Invalid algorithm option. Expected ['brute', 'voronoi', 'hierarchical'], got {}\".format(self.algorithm))\n",
    "        return index\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'numpy.ndarray'>\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_components = np.array(2)\n",
    "print(n_components)\n",
    "print(type(n_components))\n",
    "print(np.issubdtype(np.int, np.integer))\n",
    "print(np.issubdtype(int, np.integer))\n",
    "print(np.issubdtype(np.uint8, np.integer))\n",
    "print(np.issubdtype(int, np.integer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "index = torch.randint(low=0, high=5, size=(16,)).to(torch.long)\n",
    "F.one_hot(index, num_classes=6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "F.one_hot(torch.tensor(1), num_classes=6).repeat(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "unlabeled_loss = 0\n",
    "unlabeled_loss = unlabeled_loss + torch.Tensor([9.])\n",
    "unlabeled_loss += torch.Tensor([9.])\n",
    "unlabeled_loss += torch.Tensor([9.])\n",
    "print(torch.mean(unlabeled_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32, 12, 15, 97, 16, 57, 69, 98, 73, 33, 79, 58, 50, 89, 53, 95, 27, 43,\n",
       "        28, 30, 76, 48, 66, 51, 26, 40, 45, 10, 86, 39, 83, 13, 67, 52, 72, 25,\n",
       "        90, 64, 68, 55,  3, 38, 62, 96, 74, 24, 35,  2, 61, 88, 63, 37, 49, 84,\n",
       "         5, 70,  6, 54,  0, 85, 20, 71, 44,  8,  9, 91, 41, 81, 99, 36, 60, 82,\n",
       "        93, 56, 31,  7, 92, 75, 11, 29, 19, 59, 47, 21, 22, 77, 65,  1, 17, 87,\n",
       "        94,  4, 23, 34, 46, 14, 78, 42, 80, 18])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.5367e-01, -9.8013e-01, -3.9470e-01,  ..., -4.8490e-01,\n",
       "            1.1265e+00,  2.8259e-01],\n",
       "          [ 1.1218e+00,  6.5208e-01, -5.2254e-01,  ..., -7.9252e-01,\n",
       "            1.6929e+00,  8.5924e-01],\n",
       "          [ 1.0971e+00, -9.5824e-01,  4.1752e-01,  ..., -1.0960e-01,\n",
       "            1.0058e+00,  3.3421e-02],\n",
       "          ...,\n",
       "          [-6.4956e-01,  2.8781e+00, -6.5023e-01,  ..., -5.5994e-01,\n",
       "           -1.1134e+00, -4.5962e-01],\n",
       "          [-7.9998e-01, -6.8363e-02,  8.0596e-01,  ..., -1.5374e-01,\n",
       "            7.8103e-01, -3.7725e-01],\n",
       "          [ 3.1900e-01,  5.2063e-01, -1.3744e-01,  ...,  8.4876e-01,\n",
       "           -4.5599e-01, -1.4758e+00]]],\n",
       "\n",
       "\n",
       "        [[[-4.2002e-01,  6.3339e-01, -2.2178e+00,  ..., -1.0256e+00,\n",
       "            7.0947e-01, -9.1578e-02],\n",
       "          [ 4.7112e-01,  1.4571e-01,  3.2345e-01,  ..., -2.1362e-01,\n",
       "            4.2283e-01,  9.6770e-02],\n",
       "          [-6.7567e-01,  4.2544e-01, -6.5585e-01,  ..., -1.2827e+00,\n",
       "            8.4240e-01,  5.5083e-01],\n",
       "          ...,\n",
       "          [ 8.2635e-01, -2.1638e-01, -6.1541e-01,  ..., -1.3031e+00,\n",
       "            1.8444e+00, -6.3845e-01],\n",
       "          [-4.9851e-01,  1.6009e-02, -3.1219e-01,  ..., -1.5929e+00,\n",
       "           -4.8661e-01, -5.2245e-02],\n",
       "          [-1.2376e+00, -2.5077e+00, -7.0487e-01,  ..., -1.2161e+00,\n",
       "           -1.6001e-01, -1.8258e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.4407e+00, -1.7690e+00, -3.9172e-01,  ..., -2.2602e+00,\n",
       "            3.1858e-01,  3.8657e-01],\n",
       "          [ 4.1532e-01,  1.6746e+00, -4.9159e-01,  ..., -1.0629e+00,\n",
       "           -9.0035e-03,  3.0422e-01],\n",
       "          [ 9.0946e-01,  6.1705e-03, -3.8508e-01,  ...,  1.3093e+00,\n",
       "            8.8354e-01, -1.7715e-01],\n",
       "          ...,\n",
       "          [-1.0190e+00,  1.4837e-02,  1.2230e+00,  ...,  2.4436e+00,\n",
       "           -1.2409e+00,  2.9318e-01],\n",
       "          [ 7.4581e-01,  3.8688e-01, -7.1494e-01,  ..., -1.7845e-01,\n",
       "           -1.9284e+00, -1.5682e+00],\n",
       "          [ 9.3621e-01, -1.5184e-01,  2.0991e+00,  ..., -3.4331e-01,\n",
       "           -4.3177e-01, -8.1915e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.2978e+00,  5.5841e-01, -1.2012e+00,  ...,  5.6781e-01,\n",
       "            8.1638e-01,  5.7432e-01],\n",
       "          [-1.3704e+00,  5.0229e-01, -1.3559e+00,  ...,  7.8768e-01,\n",
       "            5.5635e-01,  4.4159e-01],\n",
       "          [-7.9483e-02, -1.7585e-01,  2.8971e-01,  ...,  1.3624e+00,\n",
       "           -8.2570e-01,  7.8915e-02],\n",
       "          ...,\n",
       "          [-1.2700e-01, -3.0164e-01, -3.0927e-02,  ..., -5.4662e-01,\n",
       "            5.5769e-02, -7.9472e-01],\n",
       "          [ 1.6161e+00,  1.5053e+00,  2.0936e+00,  ...,  1.5013e+00,\n",
       "            4.5405e-02,  9.6755e-01],\n",
       "          [-2.4022e-01,  2.4177e+00,  4.3876e-01,  ..., -4.1267e-01,\n",
       "           -8.9943e-01, -5.8096e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.5415e+00,  5.8201e-01, -1.2917e+00,  ...,  1.2221e-01,\n",
       "           -3.9424e-01,  1.1904e+00],\n",
       "          [-1.3013e+00, -1.4455e-01, -1.0621e+00,  ...,  1.4559e+00,\n",
       "           -2.4883e-01,  3.1634e-01],\n",
       "          [ 9.0087e-01,  5.6216e-01, -4.9070e-01,  ...,  6.1282e-01,\n",
       "           -5.3869e-01,  1.8806e+00],\n",
       "          ...,\n",
       "          [ 5.4432e-01, -1.5381e-01, -1.0327e+00,  ...,  7.8503e-02,\n",
       "            1.2422e+00,  3.6798e-01],\n",
       "          [-4.9347e-01, -1.6023e+00,  1.0573e-01,  ..., -7.8384e-01,\n",
       "           -4.9422e-01, -6.7900e-01],\n",
       "          [-5.0999e-01,  1.9470e+00, -2.8604e-03,  ..., -5.4561e-02,\n",
       "           -7.8138e-01, -7.6943e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7030e+00, -2.9450e-01, -1.1344e+00,  ..., -1.2919e+00,\n",
       "           -7.5415e-01,  1.5883e-01],\n",
       "          [-7.4265e-01,  1.4951e-01, -1.0539e+00,  ...,  1.1042e+00,\n",
       "           -5.5725e-03, -1.3875e+00],\n",
       "          [-1.0465e+00, -1.1904e+00, -7.5098e-01,  ...,  3.4152e-01,\n",
       "           -6.4805e-01, -8.9619e-01],\n",
       "          ...,\n",
       "          [ 1.2266e-01, -1.9822e+00,  1.3445e+00,  ..., -7.0534e-01,\n",
       "            1.9236e+00,  2.1630e+00],\n",
       "          [-6.3205e-01,  5.2213e-01, -1.0194e+00,  ..., -5.7329e-01,\n",
       "           -1.4241e+00, -8.0044e-01],\n",
       "          [-1.4894e+00,  1.5217e+00, -1.1197e+00,  ...,  1.1085e+00,\n",
       "            1.3799e-01,  5.6175e-01]]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn((100, 1, 486, 486))\n",
    "data[torch.randperm(100), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 16, 22, 31, 54, 62, 84, 92, 94])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.randint(low=0, high=10, size=(100,))\n",
    "torch.nonzero(targets==0)[:,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gen/.pyenv/versions/miniconda3-latest/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.range(0, 100-1)\n",
    "list(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 100\n",
    "shuffle=False\n",
    "idx = torch.randperm(L) if shuffle else torch.arange(L)\n",
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 70,  71,  72,  77,  82,  84,  86,  87,  88,  89,  92,  93,  95,\n",
       "        96, 101, 103, 104, 105, 106, 107, 110, 111, 112, 114, 115, 117,\n",
       "       120, 121, 122, 124, 125, 126, 128, 130, 132, 133, 134, 135, 136,\n",
       "       137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151,\n",
       "       152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164,\n",
       "       165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177,\n",
       "       178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190,\n",
       "       191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203,\n",
       "       204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216,\n",
       "       217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229,\n",
       "       230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242,\n",
       "       243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
       "       256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268,\n",
       "       269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
       "       282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
       "       295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "       308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320,\n",
       "       321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n",
       "       334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n",
       "       347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n",
       "       360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372,\n",
       "       373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n",
       "       386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398,\n",
       "       399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
       "       412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424,\n",
       "       425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437,\n",
       "       438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n",
       "       451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463,\n",
       "       464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
       "       477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "       490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502,\n",
       "       503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515,\n",
       "       516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528,\n",
       "       529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541,\n",
       "       542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554,\n",
       "       555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567,\n",
       "       568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580,\n",
       "       581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593,\n",
       "       594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606,\n",
       "       607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619,\n",
       "       620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632,\n",
       "       633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645,\n",
       "       646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658,\n",
       "       659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671,\n",
       "       672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684,\n",
       "       685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697,\n",
       "       698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710,\n",
       "       711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723,\n",
       "       724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736,\n",
       "       737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749,\n",
       "       750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762,\n",
       "       763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775,\n",
       "       776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
       "       789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801,\n",
       "       802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814,\n",
       "       815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827,\n",
       "       828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840,\n",
       "       841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853,\n",
       "       854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866,\n",
       "       867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879,\n",
       "       880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892,\n",
       "       893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905,\n",
       "       906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
       "       919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931,\n",
       "       932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944,\n",
       "       945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957,\n",
       "       958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970,\n",
       "       971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983,\n",
       "       984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996,\n",
       "       997, 998, 999])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  numpy as np\n",
    "\n",
    "    \n",
    "L = 1000\n",
    "idx = torch.arange(L)\n",
    "\n",
    "data = torch.randn(L, 1, 213, 213)\n",
    "targets = torch.randint(low=0, high=10, size=(L,))\n",
    "\n",
    "targets = targets[idx]\n",
    "num_per_class = 10\n",
    "y_dim = 10\n",
    "uni_idx = np.empty(0).astype(np.integer)\n",
    "for i in range(y_dim):\n",
    "    uni_idx = np.append(uni_idx, torch.nonzero(targets==i)[:,0].numpy()[:num_per_class])\n",
    "\n",
    "rem_idx = np.array(list(set(idx.numpy()) - set(uni_idx)))\n",
    "\n",
    "rem_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False,  True, False, False,\n",
      "         True, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True, False, False, False, False, False, False,  True,\n",
      "        False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "         True, False,  True, False, False,  True, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False,  True, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False,  True, False, False,\n",
      "        False, False, False,  True,  True, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False,  True, False, False,\n",
      "         True, False, False,  True, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False,  True,  True, False,  True, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "         True, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "         True, False, False,  True, False,  True, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False,  True, False, False, False, False,  True,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False,  True, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "for i in torch.Tensor([5, 10, 16, 19]).to(torch.long):\n",
    "    print(targets==i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def get_dataloader(num_training=50000, num_labeled=3000, batch_size=200):\n",
    "\n",
    "    train = MNIST(root='./mnist',download=True)\n",
    "    train_data=(train.train_data.view(-1,784).float()/255.0)\n",
    "    train_label=train.train_labels\n",
    "\n",
    "    dataset={}\n",
    "    dataset['data']=[]\n",
    "    dataset['label']=[]\n",
    "\n",
    "    dataset['test_data']=[]\n",
    "    dataset['test_label']=[]\n",
    "\n",
    "    num_per_class=num_training//10\n",
    "    num_labeled_per_class=num_labeled//10\n",
    "    for i in range(10):\n",
    "        ind_i=torch.nonzero(train_label==i)[:,0].numpy()\n",
    "        np.random.shuffle(ind_i)\n",
    "        dataset['data'].append(train_data[ind_i[:num_per_class],:])\n",
    "        dataset['label'].append(train_label[ind_i[:num_per_class]])\n",
    "\n",
    "        dataset['test_data'].append(train_data[ind_i[num_per_class:],:])\n",
    "        dataset['test_label'].append(train_label[ind_i[num_per_class:]])\n",
    "\n",
    "\n",
    "    datas=torch.cat(dataset['data'],0)\n",
    "    labels=torch.cat(dataset['label'],0)\n",
    "    labels=torch.zeros(labels.size(0),10).scatter_(1, labels.view(-1,1), 1)\n",
    "\n",
    "    # dataset={}\n",
    "\n",
    "    dataset['labeled_data']=datas[torch.Tensor(np.concatenate([np.arange(i*num_per_class,i*num_per_class+num_labeled_per_class) for i in range(10)],0)).long(),:]\n",
    "    dataset['labeled_label']=labels[torch.Tensor(np.concatenate([np.arange(i*num_per_class,i*num_per_class+num_labeled_per_class) for i in range(10)],0)).long(),:]\n",
    "\n",
    "    dataset['unlabeled_data']=datas[torch.Tensor(np.concatenate([np.arange(i*num_per_class+num_labeled_per_class,(i+1)*num_per_class) for i in range(10)],0)).long(),:]\n",
    "    dataset['unlabeled_label']=labels[torch.Tensor(np.concatenate([np.arange(i*num_per_class+num_labeled_per_class,(i+1)*num_per_class) for i in range(10)],0)).long(),:]\n",
    "\n",
    "    dataset['test_data']=torch.cat(dataset['test_data'],0)\n",
    "    dataset['test_label']=torch.cat(dataset['test_label'],0)\n",
    "\n",
    "    dataloader={}\n",
    "    dataloader['labeled'] = DataLoader(TensorDataset(dataset['labeled_data'], dataset['labeled_label']),\n",
    "                                       batch_size=num_labeled // (num_training // batch_size), shuffle=True,\n",
    "                                       num_workers=4)\n",
    "\n",
    "    dataloader['unlabeled'] = DataLoader(TensorDataset(dataset['unlabeled_data'], dataset['unlabeled_label']),\n",
    "                                       batch_size=batch_size-num_labeled // (num_training // batch_size), shuffle=True,\n",
    "                                       num_workers=4)\n",
    "\n",
    "    dataloader['test'] = DataLoader(TensorDataset(dataset['test_data'],dataset['test_label']),\n",
    "                                    batch_size=500,shuffle=False,num_workers=4)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def bce_loss(inputs, targets):\n",
    "    loss = F.binary_cross_entropy(inputs, targets, reduction='none').view(inputs.shape[0], -1)\n",
    "    return loss\n",
    "\n",
    "def _log_norm(x, mean=None, var=None):\n",
    "    if mean is None:\n",
    "        mean = torch.zeros_like(x)\n",
    "    if var is None:\n",
    "        var = torch.ones_like(x)\n",
    "    return -0.5 * (torch.log(2.0 * np.pi * var) + torch.pow(x - mean, 2) / var )\n",
    "\n",
    "def log_norm_kl(x, mean, var, mean_=None, var_=None):\n",
    "    log_p = _log_norm(x, mean, var)\n",
    "    log_q = _log_norm(x, mean_, var_)\n",
    "    loss = log_p - log_q\n",
    "    return loss\n",
    "\n",
    "def entropy(logits):\n",
    "    p = logits.softmax(-1)\n",
    "    log_p = logits.log_softmax(-1)\n",
    "    entropy = -(p * log_p)\n",
    "    return entropy\n",
    "\n",
    "def softmax_cross_entropy(input, target):\n",
    "    loss = F.cross_entropy(input, target, reduction='none')\n",
    "    return loss\n",
    "\n",
    "class Gaussian(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.features = nn.Linear(in_dim, out_dim * 2)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, reparameterize=True):\n",
    "        x = self.features(x)\n",
    "        mean, logit = torch.split(x, x.shape[1] // 2, -1)\n",
    "        var = F.softplus(logit) + self.eps\n",
    "        if reparameterize:\n",
    "            x = self._reparameterize(mean, var)\n",
    "        else:\n",
    "            x = mean\n",
    "        return x, mean, var\n",
    "    \n",
    "    def _reparameterize(self, mean, var):\n",
    "        if torch.is_tensor(var):\n",
    "            std = torch.pow(var, 0.5)\n",
    "        else:\n",
    "            std = np.sqrt(var)\n",
    "        eps = torch.randn_like(mean)\n",
    "        x = mean + eps * std\n",
    "        return x\n",
    "    \n",
    "class VAE_M2(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, z_dim=32, y_dim=10, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.z_dim = z_dim\n",
    "        self.y_dim = y_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.BatchNorm1d(256, momentum=0.01),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim, momentum=0.01),  \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.y_inference = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.BatchNorm1d(256, momentum=0.01),\n",
    "            nn.Linear(256, y_dim)\n",
    "        )\n",
    "        \n",
    "        self.z_inference = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + y_dim, 256),\n",
    "            nn.BatchNorm1d(256, momentum=0.01),\n",
    "            Gaussian(256, z_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim + y_dim, 256),\n",
    "            nn.BatchNorm1d(256, momentum=0.01),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 784),\n",
    "            nn.BatchNorm1d(784, momentum=0.01),  \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self = self.to(device)\n",
    "    \n",
    "    def forward(self, ux, lx, ly, return_loss=False, alpha=1.):\n",
    "        ux = ux.to(self.device) # (batch_size, 1, x_size, x_size)\n",
    "        lx = lx.to(self.device) # (batch_size, 1, x_size, x_size)\n",
    "        ly = ly.to(self.device) # (batch_size, )\n",
    "        \n",
    "        labeled_loss = self.labeled_loss(lx, ly, alpha)\n",
    "        unlabeled_loss = self.unlabeled_loss(ux)\n",
    "\n",
    "        if return_loss:\n",
    "            return labeled_loss + unlabeled_loss\n",
    "        return x_reconst\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = x.to(self.device) # (batch_size, 1, x_size, x_size)\n",
    "        \n",
    "        x_hidden = self.encoder(x) # (batch_size, 512 * block.expansion)\n",
    "        _, _, p_y = self.y_inference(x_hidden)\n",
    "        _, y_pred = torch.max(p_y, -1)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    # labeled loss\n",
    "    def labeled_loss(self, x, y, alpha=1.):\n",
    "\n",
    "        x_hidden = self.encoder(x) # (batch_size, 512 * block.expansion\n",
    "\n",
    "        z, z_mean, z_var = self.z_inference(torch.cat((x_hidden, y), -1)) # (batch_size, z_dim)\n",
    "        x_reconst = self.decoder(torch.cat((z, y), -1)) # (batch_size, 1, ??, ??)\n",
    "\n",
    "        log_p_x = bce_loss(x_reconst, x).sum(-1)\n",
    "        log_p_y = -np.log(1 / self.y_dim)\n",
    "        log_p_z = log_norm_kl(z, z_mean, z_var, torch.zeros_like(z), torch.ones_like(z)).sum(-1)\n",
    "\n",
    "        y_logits = self.y_inference(x_hidden)\n",
    "    \n",
    "        sup_loss = alpha * softmax_cross_entropy(y_logits, torch.argmax(y,1)).sum(-1)\n",
    "\n",
    "        loss = (log_p_x + log_p_y + log_p_z + sup_loss).mean() # batch mean\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # unlabeled loss\n",
    "    def unlabeled_loss(self, x):\n",
    "        unlabeled_loss = 0\n",
    "        x_hidden = self.encoder(x) # (batch_size, 512 * block.expansion)\n",
    "        y_logits = self.y_inference(x_hidden)\n",
    "        qy = F.softmax(y_logits, -1)\n",
    "\n",
    "        for i in range(self.y_dim):\n",
    "            qyi = qy[:, i]\n",
    "            y = F.one_hot(torch.tensor(i), num_classes=self.y_dim).repeat(x.shape[0], 1).to(self.device, dtype=torch.float32)\n",
    "            z, z_mean, z_var = self.z_inference(torch.cat((x_hidden, y), -1))\n",
    "            x_reconst = self.decoder(torch.cat((z, y), -1))\n",
    "\n",
    "            log_p_x = bce_loss(x_reconst, x).sum(-1)\n",
    "            log_p_y = -np.log(1 / self.y_dim)\n",
    "            log_p_z = log_norm_kl(z, z_mean, z_var, torch.zeros_like(z), torch.ones_like(z)).sum(-1)\n",
    "            log_q_y = torch.log(qyi + 1e-10)\n",
    "\n",
    "            unlabeled_loss += (log_p_x + log_p_y + log_p_z + log_q_y) * qyi\n",
    "\n",
    "        loss = unlabeled_loss.mean() # batch mean\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 3631.426 at epoch 1\n",
      "loss = 3754.611 at epoch 2\n",
      "loss = 3742.406 at epoch 3\n",
      "loss = 3748.849 at epoch 4\n",
      "loss = 3637.217 at epoch 5\n",
      "loss = 3634.219 at epoch 6\n",
      "loss = 3662.515 at epoch 7\n",
      "loss = 3700.556 at epoch 8\n",
      "loss = 3652.645 at epoch 9\n",
      "loss = 3853.604 at epoch 10\n",
      "loss = 3703.984 at epoch 11\n",
      "loss = 3725.702 at epoch 12\n",
      "loss = 3781.822 at epoch 13\n",
      "loss = 3600.037 at epoch 14\n",
      "loss = 3739.469 at epoch 15\n",
      "loss = 3763.411 at epoch 16\n",
      "loss = 3697.403 at epoch 17\n",
      "loss = 3659.354 at epoch 18\n",
      "loss = 3695.303 at epoch 19\n",
      "loss = 3883.104 at epoch 20\n",
      "loss = 3667.581 at epoch 21\n",
      "loss = 3709.542 at epoch 22\n",
      "loss = 3590.102 at epoch 23\n",
      "loss = 3695.595 at epoch 24\n",
      "loss = 3760.744 at epoch 25\n",
      "loss = 3824.709 at epoch 26\n",
      "loss = 3753.752 at epoch 27\n",
      "loss = 3705.874 at epoch 28\n",
      "loss = 3762.392 at epoch 29\n",
      "loss = 3846.200 at epoch 30\n",
      "loss = 3730.970 at epoch 31\n",
      "loss = 3699.435 at epoch 32\n",
      "loss = 3704.175 at epoch 33\n",
      "loss = 3689.582 at epoch 34\n",
      "loss = 3704.817 at epoch 35\n",
      "loss = 3663.893 at epoch 36\n",
      "loss = 3599.257 at epoch 37\n",
      "loss = 3666.012 at epoch 38\n",
      "loss = 3711.226 at epoch 39\n",
      "loss = 3675.994 at epoch 40\n",
      "loss = 3853.304 at epoch 41\n",
      "loss = 3672.812 at epoch 42\n",
      "loss = 3670.467 at epoch 43\n",
      "loss = 3698.552 at epoch 44\n",
      "loss = 3642.531 at epoch 45\n",
      "loss = 3614.875 at epoch 46\n",
      "loss = 3638.073 at epoch 47\n",
      "loss = 3800.910 at epoch 48\n",
      "loss = 3766.280 at epoch 49\n",
      "loss = 3746.349 at epoch 50\n",
      "loss = 3791.261 at epoch 51\n",
      "loss = 3752.468 at epoch 52\n",
      "loss = 3688.500 at epoch 53\n",
      "loss = 3674.709 at epoch 54\n",
      "loss = 3776.702 at epoch 55\n",
      "loss = 3746.353 at epoch 56\n",
      "loss = 3719.542 at epoch 57\n",
      "loss = 3710.158 at epoch 58\n",
      "loss = 3800.964 at epoch 59\n",
      "loss = 3816.653 at epoch 60\n",
      "loss = 3796.889 at epoch 61\n",
      "loss = 3589.537 at epoch 62\n",
      "loss = 3769.952 at epoch 63\n",
      "loss = 3583.378 at epoch 64\n",
      "loss = 3792.069 at epoch 65\n",
      "loss = 3761.180 at epoch 66\n",
      "loss = 3652.094 at epoch 67\n",
      "loss = 3686.751 at epoch 68\n",
      "loss = 3724.385 at epoch 69\n",
      "loss = 3798.586 at epoch 70\n",
      "loss = 3725.312 at epoch 71\n",
      "loss = 3671.646 at epoch 72\n",
      "loss = 3788.050 at epoch 73\n",
      "loss = 3821.569 at epoch 74\n",
      "loss = 3797.010 at epoch 75\n",
      "loss = 3695.788 at epoch 76\n",
      "loss = 3671.363 at epoch 77\n",
      "loss = 3789.225 at epoch 78\n",
      "loss = 3875.991 at epoch 79\n",
      "loss = 3625.798 at epoch 80\n",
      "loss = 3727.222 at epoch 81\n",
      "loss = 3820.900 at epoch 82\n",
      "loss = 3847.930 at epoch 83\n",
      "loss = 3762.745 at epoch 84\n",
      "loss = 3655.902 at epoch 85\n",
      "loss = 3667.242 at epoch 86\n",
      "loss = 3753.378 at epoch 87\n",
      "loss = 3826.066 at epoch 88\n",
      "loss = 3778.808 at epoch 89\n",
      "loss = 3747.233 at epoch 90\n",
      "loss = 3769.927 at epoch 91\n",
      "loss = 3824.164 at epoch 92\n",
      "loss = 3641.045 at epoch 93\n",
      "loss = 3670.722 at epoch 94\n",
      "loss = 3701.125 at epoch 95\n",
      "loss = 3693.408 at epoch 96\n",
      "loss = 3588.145 at epoch 97\n",
      "loss = 3695.429 at epoch 98\n",
      "loss = 3737.613 at epoch 99\n",
      "loss = 3680.116 at epoch 100\n"
     ]
    }
   ],
   "source": [
    "model = VAE_M2()\n",
    "dataloader = get_dataloader()\n",
    "n_epoch = 100\n",
    "for epoch in range(1, n_epoch+1):\n",
    "    loss = 0\n",
    "    for step, (labeled_batch, unlabeled_batch) in enumerate(zip(dataloader['labeled'], dataloader['unlabeled'])):\n",
    "        lx, ly = labeled_batch\n",
    "        ux, _ = unlabeled_batch\n",
    "        loss = model(ux, lx, ly, return_loss=True, alpha=0.1 * 200)\n",
    "        loss += loss.item()\n",
    "    print(f'loss = {loss:.3f} at epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
