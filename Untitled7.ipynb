{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from net.models.resnet import ResNet\n",
    "from net.models.resvae import ResVAE_M1,  ResVAE_M2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "resnet = ResNet(block='basic')\n",
    "model= ResVAE_M1(resnet, device='cpu', hidden_dim=16, verbose=True)\n",
    "\n",
    "\n",
    "ux = torch.sigmoid(torch.randn(16, 1, 240, 240))\n",
    "lx = torch.sigmoid(torch.randn(16, 1, 241, 241))\n",
    "target = torch.randint(low=0, high=9, size=(16,))\n",
    "out = resnet(ux)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import  numpy as np\n",
    "\n",
    "eps = 1e-10\n",
    "\n",
    "def bce_loss(inputs, targets):\n",
    "    loss = F.binary_cross_entropy(inputs, targets, reduction='none').view(inputs.shape[0], -1)\n",
    "    return loss\n",
    "\n",
    "def log_norm_kl(mean, var, mean_=None, var_=None):\n",
    "    if mean_ is None:\n",
    "        mean_ = torch.zeros_like(mean)\n",
    "    if var_ is None:\n",
    "        var_ = torch.ones_like(mean)\n",
    "    loss = 0.5 * ( torch.log(var_ / var) + (var + torch.pow(mean - mean_, 2)) / var_ - 1)\n",
    "    return loss\n",
    "\n",
    "def log_standard_norm_kl(mean, var):\n",
    "    loss = 0.5 * (-torch.log(var) + torch.pow(mean, 2) + var - 1)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([200.0000,   2.1534,  53.3487,   0.7063])\n",
      "tensor([200.0000,   2.1534,  53.3487,   0.7063])\n"
     ]
    }
   ],
   "source": [
    "mean = torch.Tensor([20,0-2,10,0.1])\n",
    "var = torch.Tensor([1,2,10,0.1])\n",
    "x = mean + torch.pow(var, 0.5) * torch.randn_like(mean)\n",
    "\n",
    "l1 = log_norm_kl(mean, var, torch.zeros_like(mean), torch.ones_like(var))\n",
    "l2 = log_standard_norm_kl(mean, var)\n",
    "\n",
    "print(l1)\n",
    "print(l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.ceil(10 / 2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Quantize(nn.Module):\n",
    "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_embed = n_embed\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        embed = torch.randn(dim, n_embed)\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    def forward(self, input):\n",
    "        flatten = input.reshape(-1, self.dim)\n",
    "        dist = (\n",
    "            flatten.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * flatten @ self.embed\n",
    "            + self.embed.pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "        _, embed_ind = (-dist).max(1)\n",
    "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
    "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
    "        quantize = self.embed_code(embed_ind)\n",
    "\n",
    "        if self.training:\n",
    "            embed_onehot_sum = embed_onehot.sum(0)\n",
    "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
    "\n",
    "            dist_fn.all_reduce(embed_onehot_sum)\n",
    "            dist_fn.all_reduce(embed_sum)\n",
    "\n",
    "            self.cluster_size.data.mul_(self.decay).add_(\n",
    "                embed_onehot_sum, alpha=1 - self.decay\n",
    "            )\n",
    "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
    "            n = self.cluster_size.sum()\n",
    "            cluster_size = (\n",
    "                (self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n\n",
    "            )\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        diff = (quantize.detach() - input).pow(2).mean()\n",
    "        quantize = input + (quantize - input).detach()\n",
    "\n",
    "        return quantize, diff, embed_ind\n",
    "\n",
    "    def embed_code(self, embed_id):\n",
    "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([4, 4])\n",
      "tensor([[0.1000, 0.2000],\n",
      "        [0.3000, 0.3200],\n",
      "        [0.3000, 0.3200],\n",
      "        [0.4000, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "dim=2\n",
    "n_embed=4\n",
    "embed = torch.Tensor([[0.1,0.2,0.3,0.4],[0.2,0.3,0.32,0.1]])\n",
    "print(embed.shape)\n",
    "x = torch.Tensor([[0.16,0.22],[0.30,0.41],[0.31,0.41],[0.51,0.11]])\n",
    "flatten = x.reshape(-1, dim)\n",
    "dist = (\n",
    "    flatten.pow(2).sum(1, keepdim=True)\n",
    "    - 2 * flatten @ embed\n",
    "    + embed.pow(2).sum(0, keepdim=True)\n",
    ")\n",
    "print(dist.shape)\n",
    "z, embed_ind = (-dist).max(1)\n",
    "embed_onehot = F.one_hot(embed_ind, n_embed).type(flatten.dtype)\n",
    "embed_ind = embed_ind.view(*x.shape[:-1])\n",
    "quantize = F.embedding(embed_ind, embed.transpose(0, 1))\n",
    "print(quantize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embed = torch.randn(4, 4)\n",
    "x = torch.randn(2, 4, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5969]],\n",
      "\n",
      "         [[-0.2505]],\n",
      "\n",
      "         [[-0.4301]],\n",
      "\n",
      "         [[ 0.8040]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4399]],\n",
      "\n",
      "         [[-0.5674]],\n",
      "\n",
      "         [[-0.3614]],\n",
      "\n",
      "         [[-0.6687]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3699, -0.9325, -1.2535,  1.0990]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6362,  0.6444,  1.0440,  0.0251]]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Quantizer(nn.Module):\n",
    "    def __init__(self, e_dim, n_embed):\n",
    "        super().__init__()\n",
    "        embed = torch.randn(e_dim, n_embed)\n",
    "        self.register_buffer(\"embed\", embed) # don't optimize by training\n",
    "\n",
    "    def forward(self, x):\n",
    "        e_dim, n_embed = self.embed.shape\n",
    "        # 1. get encoded tensor x: (B, C, H, W)\n",
    "        # 2. flatten x into (B*H*W,C)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        flatten = x.reshape(-1, e_dim)\n",
    "\n",
    "        # distance = (f - e)^2 = f^2- 2*f*e + e^2\n",
    "        distance = (\n",
    "            flatten.pow(2).sum(1, keepdim=True)\n",
    "            - 2 * flatten @ self.embed\n",
    "            + self.embed.pow(2).sum(0, keepdim=True)\n",
    "        )\n",
    "        _, embed_idx = (-distance).max(1)\n",
    "        embed_onehot = F.one_hot(embed_idx, n_embed).to(flatten.dtype)\n",
    "        embed_idx = embed_idx.view(*x.shape[:-1])\n",
    "        quantize = self.embed_code(embed_idx)\n",
    "        return quantize\n",
    "\n",
    "    def embed_code(self, embed_idx):\n",
    "        return F.embedding(embed_idx, self.embed.transpose(0, 1))\n",
    "\n",
    "x = torch.randn(2, 4, 1, 1)\n",
    "quantizer = Quantizer(4, 4)\n",
    "quantizer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Reshape()\n",
      "    (4): ConvTranspose2d(512, 512, kernel_size=(8, 8), stride=(1, 1))\n",
      "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (8): Sequential(\n",
      "      (0): BasicResTransposeBlock(\n",
      "        (block): Sequential(\n",
      "          (0): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): ConvTranspose2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (connection): Sequential(\n",
      "          (0): ConvTranspose2d(512, 512, kernel_size=(1, 1), stride=(2, 2), output_padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicResTransposeBlock(\n",
      "        (block): Sequential(\n",
      "          (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): ConvTranspose2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (convt1_x): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicResTransposeBlock(\n",
      "        (block): Sequential(\n",
      "          (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): ConvTranspose2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (connection): Sequential(\n",
      "          (0): ConvTranspose2d(512, 256, kernel_size=(1, 1), stride=(2, 2), output_padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicResTransposeBlock(\n",
      "        (block): Sequential(\n",
      "          (0): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): ConvTranspose2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (convt2_x): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicResTransposeBlock(\n",
      "        (block): Sequential(\n",
      "          (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): ConvTranspose2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (connection): Sequential(\n",
      "          (0): ConvTranspose2d(256, 128, kernel_size=(1, 1), stride=(2, 2), output_padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicResTransposeBlock(\n",
      "        (block): Sequential(\n",
      "          (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): ConvTranspose2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (convt3_x): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicResTransposeBlock(\n",
      "        (block): Sequential(\n",
      "          (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): ConvTranspose2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (connection): Sequential(\n",
      "          (0): ConvTranspose2d(128, 64, kernel_size=(1, 1), stride=(2, 2), output_padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicResTransposeBlock(\n",
      "        (block): Sequential(\n",
      "          (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): ConvTranspose2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (convt4_x): Sequential(\n",
      "    (0): ConvTranspose2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (activation): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 256, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from net.models.resnet import *\n",
    "from net import layers\n",
    "import copy\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, out_planes=1, block=BasicResTransposeBlock(), num_blocks=(2, 2, 2, 2),\n",
    "                 scale_factor=8, activation=nn.Sigmoid()):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(num_blocks) == 4, 'num_blocks must be array have length of 4'\n",
    "\n",
    "        self.next_planes = 512\n",
    "        assert hasattr(block, 'compile')\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            layers.Reshape((512, 1, 1)),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=scale_factor),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            self._make_layer(512, block, num_blocks[0], stride=2)\n",
    "        )\n",
    "        self.convt1_x = nn.Sequential(\n",
    "            self._make_layer(256, block, num_blocks[1], stride=2)\n",
    "        )\n",
    "        self.convt2_x = nn.Sequential(\n",
    "            self._make_layer(128, block, num_blocks[2], stride=2)\n",
    "        )\n",
    "        self.convt3_x = nn.Sequential(\n",
    "            self._make_layer(64, block, num_blocks[3], stride=2)\n",
    "        )\n",
    "        self.convt4_x = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, out_planes, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1)\n",
    "        )\n",
    "        self.activation = activation\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, out_planes, block, num_block, stride=1):\n",
    "        assert num_block > 0\n",
    "\n",
    "        layers = []\n",
    "        layers.append(self._block(block, in_planes=self.next_planes, out_planes=out_planes, stride=stride))\n",
    "        self.next_planes = out_planes * block.expansion\n",
    "\n",
    "        for _ in range(1, num_block):\n",
    "            layers.append(self._block(block, in_planes=self.next_planes, out_planes=out_planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _block(self, block, **kwargs):\n",
    "        block = copy.deepcopy(block)\n",
    "        block.compile(**kwargs)\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.convt1_x(x)\n",
    "        x = self.convt2_x(x)\n",
    "        x = self.convt3_x(x)\n",
    "        x = self.convt4_x(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "decoder = Decoder(in_dim=64)\n",
    "print(decoder)\n",
    "x = torch.randn(16, 64)\n",
    "decoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0930, -0.0732, -0.0732, -0.0732, -0.3244],\n",
       "          [ 0.2556, -0.0579, -0.0579, -0.0579, -0.4716],\n",
       "          [ 0.2556, -0.0579, -0.0579, -0.0579, -0.4716],\n",
       "          [ 0.2556, -0.0579, -0.0579, -0.0579, -0.4716],\n",
       "          [ 0.0045, -0.1428, -0.1428, -0.1428, -0.3054]]]],\n",
       "       grad_fn=<SlowConvTranspose2DBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 1, 1)\n",
    "layer = nn.Sequential(\n",
    "    nn.Upsample(scale_factor=4),\n",
    "    nn.ConvTranspose2d(1, 1, kernel_size=2, stride=1)\n",
    ")\n",
    "for m in layer.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.kaiming_normal_(\n",
    "            m.weight, mode='fan_out', nonlinearity='relu')\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from net import models\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "resnet = models.resnet.ResNet()\n",
    "model = models.resvae.ResVAE_M2(resnet)\n",
    "# x = torch.randn(16, 1, 192, 192)\n",
    "\n",
    "# print(model(x).shape)\n",
    "\n",
    "# start = time.time()\n",
    "# for i in tqdm(range(10)):\n",
    "#     x = torch.randn(16, 64)\n",
    "#     x = model(x)\n",
    "# ellapse = time.time() - start\n",
    "# print(ellapse / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(256 / (2 ** 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 366, 366])\n",
      "torch.Size([16, 1, 366, 366])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# conv = nn.ConvTranspose1d(512, 512 * 4 * 4, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "conv = nn.Conv2d(3, 3, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "convt = nn.ConvTranspose2d(3, 1, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "conv2 = nn.Conv2d(1, 1, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "conn = nn.ConvTranspose2d(3, 1, kernel_size=1, stride=2, output_padding=1, bias=False)\n",
    "x = torch.randn(16, 3, 183, 183)\n",
    "print(conv2(convt(conv(x))).shape)\n",
    "print(conn(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1595800171"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '1080Lines', 1: '1400Ripples', 2: 'Air_Compressor', 3: 'Blip', 4: 'Chirp', 5: 'Extremely_Loud', 6: 'Helix', 7: 'Koi_Fish', 8: 'Light_Modulation', 9: 'Low_Frequency_Burst', 10: 'Low_Frequency_Lines', 11: 'No_Glitch', 12: 'None_of_the_Above', 13: 'Paired_Doves', 14: 'Power_Line', 15: 'Repeating_Blips', 16: 'Scattered_Light', 17: 'Scratchy', 18: 'Tomte', 19: 'Violin_Mode', 20: 'Wandering_Line', 21: 'Whistle'}\n",
      "{0: '1080Lines', 1: '1400Ripples', 3: 'Blip', 5: 'Extremely_Loud', 6: 'Helix', 7: 'Koi_Fish', 8: 'Light_Modulation', 9: 'Low_Frequency_Burst', 10: 'Low_Frequency_Lines', 14: 'Power_Line', 15: 'Repeating_Blips', 16: 'Scattered_Light', 17: 'Scratchy', 19: 'Violin_Mode', 21: 'Whistle'}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "import torch\n",
    "import utils as ut\n",
    "\n",
    "ROOT = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "dataset = datasets.gravityspy(root=ROOT)\n",
    "target_labels = torch.Tensor([0, 1, 3, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17, 19, 21]).to(torch.long)\n",
    "print(dataset.targets_dict)\n",
    "dataset.get_by_keys(target_labels)\n",
    "print(dataset.targets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Low.F.B'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut.acronym('Low_Frequency_Burst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.randn(100).half()\n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
